import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import networkx as nx
from collections import defaultdict
import warnings
import datetime
import logging
from PIL import Image
import matplotlib.patches as mpatches  # æ–°å¢å¯¼å…¥ï¼Œç”¨äºå›¾ä¾‹
# è®¾ç½®å…¨å±€å­—ä½“ä¸º SimHeiï¼ˆé»‘ä½“ï¼‰
plt.rcParams['font.family'] = 'SimHei'
plt.rcParams['axes.unicode_minus'] = False # è§£å†³åæ ‡è½´è´Ÿå·æ˜¾ç¤ºé—®é¢˜

warnings.filterwarnings("ignore")
dataset = 'omi'
# dataset = 'machine'

# ========== æ—¥å¿—è®¾ç½® ==========
log_dir = f'./log/{dataset}'
os.makedirs(log_dir, exist_ok=True)
current_date = datetime.datetime.now().strftime('%Y-%m-%d')
log_filename = f'{log_dir}/{current_date}.log'
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename=log_filename,
    filemode='a',
    encoding='utf-8'
)
logger = logging.getLogger(__name__)
logging.getLogger().setLevel(logging.INFO)  # å…¨å±€è®¾ç½®ä¸º INFO
logger.setLevel(logging.DEBUG)              # åªä¿ç•™è‡ªå·± logger çš„ DEBUG

logger.info("python test20250414-0.py")
logger.info("train ä¸‹ç•Œ:5% ä¸Šç•Œ:95%,pearsonç›¸å…³ç³»æ•°å–ç»å¯¹å€¼,ç”ŸæˆGifå›¾ç”»")
logger.info("===== æ—¥å¿—åˆå§‹åŒ–å®Œæˆ =====")

# ---------- å¯è§†åŒ–å‡½æ•° ----------
# æ¯ä¸ªä¼ æ„Ÿå™¨æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ï¼›èŠ‚ç‚¹ä¹‹é—´çš„è¾¹è¡¨ç¤ºä¸¤ä¸ªä¼ æ„Ÿå™¨çš„ç›¸å…³æ€§ï¼›å¼‚å¸¸è¾¹å’Œå¼‚å¸¸èŠ‚ç‚¹ç”¨é¢œè‰²çªå‡ºæ˜¾ç¤ºï¼›
def visualize_anomaly_graph(test_data, start, end, edge_thresholds, topk_neighbors,
                            ground_truth_nodes, detected_nodes, save_path):
    window = test_data[start:end+1]
    corr = np.corrcoef(window, rowvar=False)
    num_sensors = corr.shape[0]

    G = nx.Graph()
    for i in range(num_sensors):
        G.add_node(i + 1)

    for j in range(num_sensors):
        for k in topk_neighbors[j]:
            key = (min(j, k), max(j, k))
            if key not in edge_thresholds:
                continue
            val = corr[j, k] if j < k else corr[k, j]
            if np.isnan(val):
                continue
            low, high = edge_thresholds[key]
            is_abnormal = val < low or val > high
            G.add_edge(j + 1, k + 1, color='red' if is_abnormal else 'gray')

    edge_colors = [G[u][v]['color'] for u, v in G.edges()]

    node_colors = []
    for node in G.nodes():
        if node in ground_truth_nodes and node in detected_nodes:
            node_colors.append('purple')
        elif node in ground_truth_nodes:
            node_colors.append('blue')
        elif node in detected_nodes:
            node_colors.append('orange')
        else:
            node_colors.append('lightgray')

    plt.figure(figsize=(10, 8))
    # pos = nx.spring_layout(G, seed=42)
    pos = nx.spring_layout(G, seed=42, k=1.2, iterations=100)

    nx.draw(G, pos, with_labels=True, node_color=node_colors, edge_color=edge_colors,
            node_size=500, font_size=10, width=2)
    plt.title(f"Segment {start}-{end}")

    # æ·»åŠ å›¾ä¾‹
    legend_elements = [
        mpatches.Patch(color='orange', label='æ£€æµ‹å¼‚å¸¸'),
        mpatches.Patch(color='blue', label='GTå¼‚å¸¸'),   # ground truthå¼‚å¸¸
        mpatches.Patch(color='purple', label='æ­£ç¡®æ£€æµ‹'),
        mpatches.Patch(color='lightgray', label='æ­£å¸¸'),
        mpatches.Patch(color='red', label='å¼‚å¸¸è¾¹'),
        mpatches.Patch(color='gray', label='æ­£å¸¸è¾¹')
    ]
    plt.legend(handles=legend_elements, loc='best', fontsize='small')

    plt.savefig(save_path)
    plt.close()

'''
èŠ‚ç‚¹é¢œè‰²ï¼š
ğŸŸ  orange  -> æ£€æµ‹å¼‚å¸¸
ğŸ”µ blue    -> å®é™…å¼‚å¸¸
ğŸŸª purple  -> GT + æ£€æµ‹éƒ½å¼‚å¸¸
âšª gray    -> æ­£å¸¸

è¾¹é¢œè‰²ï¼š
ğŸ”´ red     -> å¼‚å¸¸è¾¹
âš« gray    -> æ­£å¸¸è¾¹
'''

def create_gif_from_images(image_folder, gif_path, duration=500):
    images = []
    for filename in sorted(os.listdir(image_folder)):
        if filename.endswith(".png"):
            img_path = os.path.join(image_folder, filename)
            images.append(Image.open(img_path))
    if images:
        images[0].save(gif_path, save_all=True, append_images=images[1:], duration=duration, loop=0)

# ---------- å•ä¸ªæ•°æ®é›†è¯„ä¼°å‡½æ•° ----------
def evaluate_omi_dataset_topk(train_data, test_data, test_labels, gt_txt_path,
                              window_size=5, score_threshold=3.0, top_k=5, save_fig_dir=None):
    num_sensors = train_data.shape[1]
    edge_corrs = defaultdict(list)

    for i in range(train_data.shape[0] - window_size + 1):
        window = train_data[i:i + window_size]
        corr = np.corrcoef(window, rowvar=False)
        for j in range(num_sensors):
            for k in range(j + 1, num_sensors):
                val = abs(corr[j, k])
                if not np.isnan(val):
                    edge_corrs[(j, k)].append(val)

    edge_thresholds = {
        (j, k): (np.percentile(vals, 5), np.percentile(vals, 95))
        for (j, k), vals in edge_corrs.items()
    }

    avg_corr = np.zeros((num_sensors, num_sensors))
    for (j, k), vals in edge_corrs.items():
        mean_val = np.mean(vals)
        avg_corr[j, k] = mean_val
        avg_corr[k, j] = mean_val

    topk_neighbors = {
        i: set(np.argsort(-avg_corr[i])[:top_k + 1]) - {i}
        for i in range(num_sensors)
    }

    def detect_anomalous_nodes(start, end):
        abnormal_score = defaultdict(float)
        for i in range(start, end - window_size + 2):
            window = test_data[i:i + window_size]
            corr = np.corrcoef(window, rowvar=False)
            for j in range(num_sensors):
                for k in topk_neighbors[j]:
                    key = (min(j, k), max(j, k))
                    if key not in edge_thresholds:
                        continue
                    val = corr[j, k] if j < k else corr[k, j]
                    if np.isnan(val):
                        continue
                    low, high = edge_thresholds[key]
                    if val < low or val > high:
                        deviation = max(abs(val - low), abs(val - high))
                        abnormal_score[j] += deviation
                        abnormal_score[k] += deviation
        return {node + 1 for node, score in abnormal_score.items() if score >= score_threshold}

    gt_segments = []
    with open(gt_txt_path, 'r') as f:
        for line in f:
            time_range, sensors_str = line.strip().split(':')
            start, end = map(int, time_range.split('-'))
            sensors = list(map(int, sensors_str.split(',')))
            gt_segments.append({'start': start, 'end': end, 'sensors': sensors})

    if save_fig_dir:
        os.makedirs(save_fig_dir, exist_ok=True)

    total_TP = total_FP = total_FN = 0
    for segment in gt_segments:
        start, end = segment['start'], segment['end']
        gt_sensors = set(segment['sensors'])
        predicted = detect_anomalous_nodes(start, end)

        TP = len(predicted & gt_sensors)
        FP = len(predicted - gt_sensors)
        FN = len(gt_sensors - predicted)

        total_TP += TP
        total_FP += FP
        total_FN += FN

        if save_fig_dir:
            save_path = os.path.join(save_fig_dir, f"{start}-{end}.png")
            visualize_anomaly_graph(
                test_data, start, end, edge_thresholds, topk_neighbors,
                ground_truth_nodes=gt_sensors,
                detected_nodes=predicted,
                save_path=save_path
            )

    precision = total_TP / (total_TP + total_FP) if total_TP + total_FP > 0 else 0
    recall = total_TP / (total_TP + total_FN) if total_TP + total_FN > 0 else 0
    f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

    return precision, recall, f1_score

# ---------- æ±‡æ€»æœ€ä¼˜ç»“æœ + å…¨å±€æŒ‡æ ‡ ----------
def summarize_best_results(results_df):
    best_per_dataset = results_df.sort_values(by='f1_score', ascending=False).groupby('dataset').first().reset_index()

    total_TP = total_FP = total_FN = 0
    for _, row in best_per_dataset.iterrows():
        precision = row['precision']
        recall = row['recall']
        TP = 100
        FP = int(round((TP / precision) - TP)) if precision > 0 else 0
        FN = int(round((TP / recall) - TP)) if recall > 0 else 0
        total_TP += TP
        total_FP += FP
        total_FN += FN

    overall_precision = total_TP / (total_TP + total_FP)
    overall_recall = total_TP / (total_TP + total_FN)
    overall_f1 = 2 * overall_precision * overall_recall / (overall_precision + overall_recall)

    logger.info("\n===== æ¯ä¸ªæ•°æ®é›†æœ€ä¼˜ç»„åˆ =====\n" + best_per_dataset.to_string(index=False))
    logger.info(f"\n===== å…¨éƒ¨æ•°æ®é›†æŒ‡æ ‡ =====\nPrecision: {overall_precision:.4f}  Recall: {overall_recall:.4f}  F1: {overall_f1:.4f}")

    return best_per_dataset, {
        'overall_precision': overall_precision,
        'overall_recall': overall_recall,
        'overall_f1_score': overall_f1
    }

# ---------- ä¸»æµç¨‹ ----------
if __name__ == "__main__":
    start = datetime.datetime.now()
    print(f"å¼€å§‹æ—¶é—´: {start.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"å¼€å§‹æ—¶é—´: {start.strftime('%Y-%m-%d %H:%M:%S')}")

    txt_files = [f for f in os.listdir('./data/interpretation_label') if f.startswith(f"{dataset}-") and f.endswith(".txt")]
    all_results = []

    for txt_file in txt_files:
        base = txt_file.replace('.txt', '')
        train_path = os.path.join('./data/processed', f"{base}_train.pkl")
        test_path = os.path.join('./data/processed', f"{base}_test.pkl")
        label_path = os.path.join('./data/processed', f"{base}_test_label.pkl")
        gt_txt_path = os.path.join('./data/interpretation_label', txt_file)

        if not (os.path.exists(train_path) and os.path.exists(test_path) and os.path.exists(label_path)):
            logger.warning(f"[{base}] ç¼ºå°‘æ–‡ä»¶ï¼Œè·³è¿‡")
            continue

        train_data = pd.read_pickle(train_path)
        test_data = pd.read_pickle(test_path)
        test_labels = pd.read_pickle(label_path)

        save_fig_dir = os.path.join('./figures', dataset, base)
        logger.info(f"[{base}] å¼€å§‹å¼‚å¸¸æ£€æµ‹å¹¶ç”Ÿæˆå›¾åƒ...")

        precision, recall, f1 = evaluate_omi_dataset_topk(
            train_data, test_data, test_labels, gt_txt_path,
            window_size=5, score_threshold=3.0, top_k=5,
            save_fig_dir=save_fig_dir
        )

        gif_path = os.path.join('./figures', dataset, f"{base}.gif")
        create_gif_from_images(save_fig_dir, gif_path, duration=600)

        logger.info(f"[{base}] Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}")
        all_results.append({
            'dataset': base,
            'precision': precision,
            'recall': recall,
            'f1_score': f1
        })

    results_df = pd.DataFrame(all_results)
    best_results, overall = summarize_best_results(results_df)

    print("\n===== æ¯ä¸ªæ•°æ®é›†æœ€ä¼˜å‚æ•°ç»„åˆ =====")
    print(best_results[['dataset', 'precision', 'recall', 'f1_score']])

    print("\n===== å…¨éƒ¨æ•°æ®é›†çš„æ•´ä½“æŒ‡æ ‡ =====")
    print(f"Precision: {overall['overall_precision']:.4f}")
    print(f"Recall:    {overall['overall_recall']:.4f}")
    print(f"F1-score:  {overall['overall_f1_score']:.4f}")

    end = datetime.datetime.now()
    print(f"ç»“æŸæ—¶é—´: {end.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"ç»“æŸæ—¶é—´: {end.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"è¿è¡Œå®Œæˆï¼Œç”¨æ—¶ï¼š{end - start}")
