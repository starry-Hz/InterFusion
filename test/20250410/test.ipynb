{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件: omi-1.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-10.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-11.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-12.txt,train.pkl大小: (7291, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-2.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-3.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-4.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-5.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-6.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-7.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-8.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n",
      "文件: omi-9.txt,train.pkl大小: (8640, 19),test.pkl大小: (4320, 19),test_label.pkl: (4320, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "txt_files = [f for f in os.listdir(\"data/interpretation_label\") if f.startswith(\"omi-\") and f.endswith(\".txt\")]\n",
    "# 遍历每个文件，查看其数据形状\n",
    "for txt_filename in txt_files:\n",
    "    prefix = txt_filename.split(\".\")[0]\n",
    "    train_data_path = os.path.join(\"data/processed\", f\"{prefix}_train.pkl\")\n",
    "    test_data_path = os.path.join(\"data/processed\", f\"{prefix}_test.pkl\")\n",
    "    test_labels_path = os.path.join(\"data/processed\", f\"{prefix}_test_label.pkl\")\n",
    "    \n",
    "    train_data = pd.read_pickle(train_data_path)\n",
    "    test_data = pd.read_pickle(test_data_path)\n",
    "    test_labels = pd.read_pickle(test_labels_path)\n",
    "    # print(f\"test_data type: {type(test_data)},test_labels type: {type(test_labels)}\")\n",
    "    # 为什么是通过pd读的，但是shape是ndarray？ 回答：因为在读取时，pandas会自动将数据转换为numpy数组格式\n",
    "    # 转换为DataFrame格式\n",
    "    if isinstance(train_data, np.ndarray):\n",
    "        train_data = pd.DataFrame(train_data)\n",
    "    if isinstance(test_data, np.ndarray):\n",
    "        test_data = pd.DataFrame(test_data)\n",
    "    if isinstance(test_labels, np.ndarray):\n",
    "        test_labels = pd.DataFrame(test_labels, columns=[\"label\"])\n",
    "    elif isinstance(test_labels, pd.Series):\n",
    "        test_labels = test_labels.to_frame(name=\"label\")\n",
    "\n",
    "    # print(f\"文件: {txt_filename},test.pkl大小: {test_data.shape},test_label.pkl: {test_labels.shape}\")\n",
    "    print(f\"文件: {txt_filename},train.pkl大小: {train_data.shape},test.pkl大小: {test_data.shape},test_label.pkl: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import pearsonr\n",
    "from dtaidistance import dtw\n",
    "import networkx as nx\n",
    "from gensim.models import Word2Vec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "# 配置参数\n",
    "class Config:\n",
    "    window_size = 30\n",
    "    stride = 5  # 增大步长以减少计算量\n",
    "    similarity_threshold = 0.6\n",
    "    deepwalk_dim = 64\n",
    "    num_walks = 10\n",
    "    walk_length = 20\n",
    "    anomaly_threshold = 0.5\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. 数据加载和预处理工具函数\n",
    "def load_data(base_path=\"data/processed\"):\n",
    "    data_files = {\n",
    "        'train': 'omi-1_train.pkl',\n",
    "        'test': 'omi-1_test.pkl',\n",
    "        'test_label': 'omi-1_test_label.pkl'\n",
    "    }\n",
    "    return {k: pickle.load(open(os.path.join(base_path, v), 'rb')) for k, v in data_files.items()}\n",
    "\n",
    "def parse_interpretation(file_path):\n",
    "    anomalies = defaultdict(list)\n",
    "    with open(file_path) as f:\n",
    "        for line in f:\n",
    "            if ':' not in line:\n",
    "                continue\n",
    "            range_part, sensors_part = line.strip().split(':')\n",
    "            start_end = range_part.split('-')\n",
    "            if len(start_end) != 2:\n",
    "                continue\n",
    "            start, end = map(int, start_end)\n",
    "            sensors = list(map(int, sensors_part.split(',')))\n",
    "            for ts in range(start, end+1):\n",
    "                anomalies[ts] = sensors\n",
    "    return dict(anomalies)\n",
    "\n",
    "# 2. 滑动窗口处理\n",
    "def create_windows(data, window_size, stride):\n",
    "    windows = []\n",
    "    n_samples, n_features = data.shape\n",
    "    for i in range(0, n_samples - window_size + 1, stride):\n",
    "        windows.append(data[i:i+window_size, :])\n",
    "    return np.array(windows)\n",
    "\n",
    "# 3. 图构建工具函数\n",
    "def calculate_combined_similarity(window1, window2):\n",
    "    flat1 = window1.flatten()\n",
    "    flat2 = window2.flatten()\n",
    "    \n",
    "    # Pearson相关系数\n",
    "    try:\n",
    "        pearson_corr, _ = pearsonr(flat1, flat2)\n",
    "    except:\n",
    "        pearson_corr = 0\n",
    "    \n",
    "    # DTW距离 (加速计算)\n",
    "    dtw_dist = dtw.distance_fast(flat1, flat2)\n",
    "    dtw_sim = 1 / (1 + dtw_dist)\n",
    "    \n",
    "    return 0.7 * pearson_corr + 0.3 * dtw_sim\n",
    "\n",
    "def build_adjacency_matrix(windows, threshold):\n",
    "    n = len(windows)\n",
    "    adj = np.zeros((n, n))\n",
    "    \n",
    "    # 只计算局部连接以节省时间\n",
    "    for i in tqdm(range(n), desc=\"Building graph\"):\n",
    "        for j in range(max(0, i-50), min(n, i+50)):\n",
    "            if i != j and adj[i,j] == 0:\n",
    "                sim = calculate_combined_similarity(windows[i], windows[j])\n",
    "                if sim > threshold:\n",
    "                    adj[i,j] = sim\n",
    "                    adj[j,i] = sim\n",
    "    return adj\n",
    "\n",
    "# 4. DeepWalk实现\n",
    "def deepwalk(G, num_walks, walk_length, embedding_size):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    \n",
    "    for _ in range(num_walks):\n",
    "        np.random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walk = [node]\n",
    "            while len(walk) < walk_length:\n",
    "                curr = walk[-1]\n",
    "                neighbors = list(G.neighbors(curr))\n",
    "                if neighbors:\n",
    "                    walk.append(np.random.choice(neighbors))\n",
    "                else:\n",
    "                    break\n",
    "            walks.append([str(x) for x in walk])\n",
    "    \n",
    "    model = Word2Vec(\n",
    "        walks, \n",
    "        vector_size=embedding_size, \n",
    "        window=5, \n",
    "        min_count=0, \n",
    "        sg=1, \n",
    "        workers=4,\n",
    "        epochs=10\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# 5. 异常检测模型\n",
    "class AnomalyDetector(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_model(embeddings, num_sensors, epochs=20):\n",
    "    model = AnomalyDetector(\n",
    "        input_dim=Config.deepwalk_dim,\n",
    "        hidden_dim=128,\n",
    "        output_dim=num_sensors\n",
    "    ).to(Config.device)\n",
    "    \n",
    "    # 正常样本的标签是全0\n",
    "    dummy_labels = torch.zeros((len(embeddings), num_sensors), device=Config.device)\n",
    "    train_data = torch.FloatTensor(embeddings).to(Config.device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(train_data)\n",
    "        loss = criterion(outputs, dummy_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    return model\n",
    "\n",
    "# 6. 评估函数\n",
    "def evaluate_results(test_outputs, interpretation, stride):\n",
    "    results = {\n",
    "        'timestamp': [],\n",
    "        'true_sensors': [],\n",
    "        'pred_sensors': [],\n",
    "        'correct': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': []\n",
    "    }\n",
    "    \n",
    "    # 按时间戳统计\n",
    "    all_timestamps = sorted(interpretation.keys())\n",
    "    for ts in all_timestamps:\n",
    "        window_idx = ts // stride\n",
    "        if window_idx >= len(test_outputs):\n",
    "            continue\n",
    "            \n",
    "        sensor_scores = test_outputs[window_idx]\n",
    "        pred_sensors = np.where(sensor_scores > Config.anomaly_threshold)[0] + 1  # 传感器从1开始编号\n",
    "        true_sensors = interpretation[ts]\n",
    "        \n",
    "        tp = len(set(true_sensors) & set(pred_sensors))\n",
    "        fp = len(set(pred_sensors) - set(true_sensors))\n",
    "        fn = len(set(true_sensors) - set(pred_sensors))\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        results['timestamp'].append(ts)\n",
    "        results['true_sensors'].append(true_sensors)\n",
    "        results['pred_sensors'].append(pred_sensors.tolist())\n",
    "        results['correct'].append(tp)\n",
    "        results['precision'].append(precision)\n",
    "        results['recall'].append(recall)\n",
    "        results['f1'].append(f1)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 主流程\n",
    "def process_file(interpretation_file):\n",
    "    print(f\"\\nProcessing {interpretation_file}...\")\n",
    "    \n",
    "    # 1. 加载数据\n",
    "    data = load_data()\n",
    "    interpretation = parse_interpretation(os.path.join(\"data/interpretation_label\", interpretation_file))\n",
    "    \n",
    "    # 2. 创建滑动窗口\n",
    "    train_windows = create_windows(data['train'], Config.window_size, Config.stride)\n",
    "    test_windows = create_windows(data['test'], Config.window_size, Config.stride)\n",
    "    print(f\"Train windows: {train_windows.shape}, Test windows: {test_windows.shape}\")\n",
    "    \n",
    "    # 3. 构建图\n",
    "    adj_matrix = build_adjacency_matrix(train_windows, Config.similarity_threshold)\n",
    "    G = nx.from_numpy_array(adj_matrix)\n",
    "    \n",
    "    # 4. DeepWalk嵌入\n",
    "    print(\"Running DeepWalk...\")\n",
    "    model = deepwalk(G, Config.num_walks, Config.walk_length, Config.deepwalk_dim)\n",
    "    embeddings = np.array([model.wv[str(i)] for i in range(len(train_windows))])\n",
    "    \n",
    "    # 5. 训练异常检测模型\n",
    "    print(\"Training anomaly detector...\")\n",
    "    detector = train_model(embeddings, data['train'].shape[1])\n",
    "    \n",
    "    # 6. 测试集处理\n",
    "    test_embeddings = []\n",
    "    for window in tqdm(test_windows, desc=\"Processing test windows\"):\n",
    "        # 简化版: 使用最近邻\n",
    "        similarities = [calculate_combined_similarity(window, train_windows[i]) \n",
    "                       for i in range(min(1000, len(train_windows)))]  # 限制数量加速计算\n",
    "        most_similar = np.argmax(similarities)\n",
    "        test_embeddings.append(embeddings[most_similar])\n",
    "    test_embeddings = np.array(test_embeddings)\n",
    "    \n",
    "    # 7. 预测\n",
    "    with torch.no_grad():\n",
    "        test_outputs = detector(torch.FloatTensor(test_embeddings).to(Config.device)).cpu().numpy()\n",
    "    \n",
    "    # 8. 评估\n",
    "    results_df = evaluate_results(test_outputs, interpretation, Config.stride)\n",
    "    \n",
    "    # 计算整体指标\n",
    "    avg_precision = np.mean(results_df['precision'])\n",
    "    avg_recall = np.mean(results_df['recall'])\n",
    "    avg_f1 = np.mean(results_df['f1'])\n",
    "    \n",
    "    print(f\"\\nEvaluation for {interpretation_file}:\")\n",
    "    print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "    print(f\"Average F1: {avg_f1:.4f}\")\n",
    "    \n",
    "    # 保存详细结果\n",
    "    output_dir = \"results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    result_file = os.path.join(output_dir, f\"results_{interpretation_file.replace('.txt', '.csv')}\")\n",
    "    results_df.to_csv(result_file, index=False)\n",
    "    print(f\"Detailed results saved to {result_file}\")\n",
    "    \n",
    "    return {\n",
    "        'file': interpretation_file,\n",
    "        'avg_precision': avg_precision,\n",
    "        'avg_recall': avg_recall,\n",
    "        'avg_f1': avg_f1\n",
    "    }\n",
    "\n",
    "# 主函数\n",
    "def main():\n",
    "    # 获取所有解释标签文件\n",
    "    txt_files = [f for f in os.listdir(\"data/interpretation_label\") \n",
    "                 if f.startswith(\"omi-\") and f.endswith(\".txt\")]\n",
    "    \n",
    "    if not txt_files:\n",
    "        print(\"No interpretation files found in data/interpretation_label/\")\n",
    "        return\n",
    "    \n",
    "    # 处理所有文件\n",
    "    summary_results = []\n",
    "    for file in txt_files:\n",
    "        try:\n",
    "            result = process_file(file)\n",
    "            summary_results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {str(e)}\")\n",
    "    \n",
    "    # 输出汇总结果\n",
    "    print(\"\\n=== Summary Results ===\")\n",
    "    summary_df = pd.DataFrame(summary_results)\n",
    "    print(summary_df)\n",
    "    \n",
    "    # 保存汇总结果\n",
    "    if not os.path.exists(\"results\"):\n",
    "        os.makedirs(\"results\")\n",
    "    summary_df.to_csv(\"results/summary_results.csv\", index=False)\n",
    "    print(\"\\nAll processing completed. Summary results saved to results/summary_results.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interfusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
